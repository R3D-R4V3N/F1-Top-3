# F1-Forecast

F1-Forecast is a small project that predicts which drivers will finish in the top 3 of a Formula 1 race. Data is pulled from two public APIs, cleaned into a single dataset and used to train a classification model. A Streamlit dashboard shows predictions and performance.

## Repository structure

| File | Purpose |
| --- | --- |
| `fetch_f1_data.py` | Download raw data from the OpenF1 and Jolpica APIs. Weather and session data come from OpenF1, while historical race and qualifying data come from Jolpica (an Ergast-compatible API). Functions `get_lap_data()`, `get_pitstop_data()`, `fetch_openf1_data()` and `fetch_jolpica_data()` all accept a `use_cache` flag. Lap and pit stop CSVs are cached under `cache/`, while the other helpers skip downloads if their output files already exist. |
| `prepare_data.py` | Merge the downloaded CSV files into `processed_data.csv`. It engineers features such as qualifying times in seconds, rolling averages per driver and constructor, weather information and custom interaction features. |
| `eda_f1.py` | Simple exploratory analysis on the raw CSV files. |
| `train_model.py` | Train the main RandomForest pipeline using the processed data. Hyperparameters are tuned with `GridSearchCV`. |
| `train_model_lgbm.py` | Alternative model using LightGBM. |
| `train_model_nested_cv.py` | Example of nested cross‑validation for more robust evaluation. |
| `train_model_xgb.py` | Experimental model using XGBoost. |
| `export_model.py` | Calls `build_and_train_pipeline()` from the chosen training script, refits the best estimator on the entire dataset and saves the result to `f1_top3_pipeline.joblib`. Use `--algo {rf,lgbm,xgb}` to select the algorithm. |
| `infer.py` | Recomputes features with past races only and calls `inference_for_date()` to generate predictions for a chosen race date. |
| `streamlit_app.py` | Streamlit dashboard to interactively explore predictions. |
| `f1_api_docs.md` | Documentation snippets of the OpenF1 and Jolpica APIs. |
| `*.csv` | Data files generated by the fetch and prepare scripts. |
| `cache/` | Cached lap time and pit stop CSVs generated by `get_lap_data()` and `get_pitstop_data()`. |

## Workflow

1. **Fetch data**
   ```bash
   python fetch_f1_data.py
   ```
   This script retrieves:
   - OpenF1 `weather` and `sessions` for the latest meeting.
   - Jolpica endpoints (`circuits`, `races`, `results`, `sprint`, `qualifying`, `driverstandings`, `constructorstandings`, `status`) for seasons ≥2022.
   The raw JSON responses are normalized to CSV files (e.g. `jolpica_results.csv`, `openf1_weather.csv`).
   Lap times and pit stops are fetched on demand via `get_lap_data()` and `get_pitstop_data()`. Their results are cached in the `cache/` directory so repeated runs avoid extra API calls. `fetch_openf1_data()` and `fetch_jolpica_data()` likewise skip downloads if the corresponding CSVs already exist when `use_cache=True`.

2. **Prepare dataset**
   ```bash
   python prepare_data.py
   ```
   Creates `processed_data.csv` by merging qualifying, race results, circuit info, sessions and weather. Important steps:
   - Convert qualifying times to seconds (`Q1_sec`, `Q2_sec`, `Q3_sec`).
   - Add date‑based features (`month`, `weekday`).
   - Compute rolling averages: previous finish position and grid position per driver, plus average constructor finish.
  - Merge weather via `session_key` and impute missing values.
  - Count on-track overtakes per driver using lap and pit stop data
    (`overtakes_count`, `weighted_overtakes`). These columns are kept for
    analysis only and are no longer fed into the model.
  - Parse driver and constructor standings to derive previous-season
    points and rank (`driver_points_prev`, `driver_rank_prev`,
    `constructor_points_prev`, `constructor_rank_prev`).
  - Create interaction features: `grid_diff`, `Q3_diff`, `grid_temp_int`.
   The final CSV contains one row per driver per race with a boolean `top3` label. `prepare_data.py` also downloads lap time and pit stop data for each race using the cached helpers (`use_cache=True`).

3. **Train model**
   ```bash
   python train_model.py
   ```
   - Selects the following feature columns:
    `grid_position`, `Q1_sec`, `Q2_sec`, `Q3_sec`, `month`, `weekday`,
    `avg_finish_pos`, `avg_grid_pos`, `avg_const_finish`, `air_temperature`,
    `track_temperature`, `grid_diff`, `Q3_diff`, `grid_temp_int`,
    `driver_points_prev`, `driver_rank_prev`,
    `constructor_points_prev`, `constructor_rank_prev`,
   `circuit_country`, `circuit_city`.
   - Numerical features are median‑imputed and scaled; categorical features are one‑hot encoded.
   - A `RandomForestClassifier` is tuned with a small parameter grid.
   - Cross‑validation uses `GroupTimeSeriesSplit` so each fold only sees earlier races and keeps entire events together.
   - Metrics such as ROC‑AUC, confusion matrix, precision/recall and mean absolute error are printed.
   - Key metrics and the learning curve results are written to `model_performance_<algo>.csv` for the Streamlit dashboard.
   - A learning curve is calculated with `sklearn.model_selection.learning_curve` to check for over‑ or underfitting.

   You can experiment with other algorithms via `train_model_lgbm.py`, `train_model_xgb.py` or `train_model_nested_cv.py`. These scripts output a confusion matrix and log their metrics—including learning curve values—to a matching `model_performance_<algo>.csv` file so the dashboard always shows the most recent training results.

4. **Export trained pipeline**
   ```bash
   python export_model.py --algo lgbm  # or rf/xgb
   ```
   Saves the best pipeline from the selected algorithm. `export_model.py`
   clones the tuned estimator, fits it on the entire dataset and writes the
   resulting pipeline to `f1_top3_pipeline.joblib`.

5. **Make predictions**
   ```bash
   python infer.py
   ```
   `infer.py` calls `inference_for_date()` which recomputes rolling features
   using data from races before the chosen date. Adjust the date in the script
   (or import the function elsewhere) to generate predictions for a specific
   race.

6. **Streamlit dashboard**
   ```bash
   streamlit run streamlit_app.py
   ```
   Provides an interactive dashboard to select a season and race, view predicted probabilities and display basic performance information.

## Updating the model for new races

When new race data becomes available:
1. Run `fetch_f1_data.py` again. The script fetches data for all seasons starting from 2022, so rerunning it will append the latest results and weather.
2. Recreate `processed_data.csv` with `prepare_data.py`.
3. Retrain the model (`train_model.py`) and export the updated pipeline with `export_model.py --algo rf|lgbm|xgb`.
4. Use `infer.py` (or import `inference_for_date()`) to generate predictions for the new race with features recalculated from prior events.

## Data sources

- **[OpenF1](https://www.openf1.org/)** – provides minute‑level weather data and session metadata.
- **[Jolpica F1 API](https://api.jolpi.ca/ergast/f1/)** – Ergast‑compatible endpoints containing historical race, qualifying and standings information.

See `f1_api_docs.md` for example queries and field descriptions.

## Requirements

This project relies on common Python data‑science packages such as `pandas`, `scikit‑learn`, `joblib`, `lightgbm`, `xgboost` and `streamlit`. Install them via pip:
```bash
pip install pandas scikit-learn joblib lightgbm xgboost streamlit
```

## Reproducing the full pipeline

From a clean checkout:
```bash
python fetch_f1_data.py
python prepare_data.py
python train_model.py
python export_model.py --algo rf  # or lgbm/xgb
python infer.py  # adjust the date inside to predict a specific race
```
Optionally run the Streamlit dashboard as described above.


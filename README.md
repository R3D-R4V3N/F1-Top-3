# F1-Forecast

F1-Forecast is a small project that predicts which drivers will finish in the top 3 of a Formula 1 race. Data is pulled from two public APIs, cleaned into a single dataset and used to train a classification model. A Streamlit dashboard shows predictions and performance.

## Repository structure

| File | Purpose |
| --- | --- |
| `fetch_f1_data.py` | Download raw data from the OpenF1 and Jolpica APIs. Weather and session data come from OpenF1, while historical race and qualifying data come from Jolpica (an Ergast-compatible API). Helper functions `get_lap_data()` and `get_pitstop_data()` (both accept a `use_cache` flag) cache their responses under `cache/`. |
| `prepare_data.py` | Merge the downloaded CSV files into `processed_data.csv`. It engineers features such as qualifying times in seconds, rolling averages per driver and constructor, weather information and custom interaction features. |
| `eda_f1.py` | Simple exploratory analysis on the raw CSV files. |
| `train_model.py` | Train the main RandomForest pipeline using the processed data. Hyperparameters are tuned with `GridSearchCV`. |
| `train_model_lgbm.py` | Alternative model using LightGBM. |
| `train_model_nested_cv.py` | Example of nested cross‑validation for more robust evaluation. |
| `train_model_xgb.py` | Experimental model using XGBoost. |
| `export_model.py` | Calls `build_and_train_pipeline()` from the chosen training script and saves the best pipeline to `f1_top3_pipeline.joblib`. Use `--algo {rf,lgbm,xgb}` to select the algorithm. |
| `infer.py` | Load the saved pipeline and generate top‑3 predictions for the most recent race in the dataset. |
| `streamlit_app.py` | Streamlit dashboard to interactively explore predictions. |
| `f1_api_docs.md` | Documentation snippets of the OpenF1 and Jolpica APIs. |
| `*.csv` | Data files generated by the fetch and prepare scripts. |
| `cache/` | Cached lap time and pit stop CSVs generated by `get_lap_data()` and `get_pitstop_data()`. |

## Workflow

1. **Fetch data**
   ```bash
   python fetch_f1_data.py
   ```
   This script retrieves:
   - OpenF1 `weather` and `sessions` for the latest meeting.
   - Jolpica endpoints (`circuits`, `races`, `results`, `sprint`, `qualifying`, `driverstandings`, `constructorstandings`, `status`) for seasons ≥2022.
   The raw JSON responses are normalized to CSV files (e.g. `jolpica_results.csv`, `openf1_weather.csv`).
   Lap times and pit stops are fetched on demand via `get_lap_data()` and `get_pitstop_data()`. Their results are cached in the `cache/` directory so repeated runs avoid extra API calls.

2. **Prepare dataset**
   ```bash
   python prepare_data.py
   ```
   Creates `processed_data.csv` by merging qualifying, race results, circuit info, sessions and weather. Important steps:
   - Convert qualifying times to seconds (`Q1_sec`, `Q2_sec`, `Q3_sec`).
   - Add date‑based features (`month`, `weekday`).
   - Compute rolling averages: previous finish position and grid position per driver, plus average constructor finish.
   - Merge weather via `session_key` and impute missing values.
   - Count on-track overtakes per driver using lap and pit stop data (`overtakes_count`).
   - Create interaction features: `grid_diff`, `Q3_diff`, `grid_temp_int`.
   The final CSV contains one row per driver per race with a boolean `top3` label. `prepare_data.py` also downloads lap time and pit stop data for each race using the cached helpers (`use_cache=True`).

3. **Train model**
   ```bash
   python train_model.py
   ```
   - Selects the following feature columns:
    `grid_position`, `Q1_sec`, `Q2_sec`, `Q3_sec`, `month`, `weekday`, `avg_finish_pos`, `avg_grid_pos`, `avg_const_finish`, `air_temperature`, `track_temperature`, `grid_diff`, `Q3_diff`, `grid_temp_int`, `overtakes_count`, `circuit_country`, `circuit_city`.
   - Numerical features are median‑imputed and scaled; categorical features are one‑hot encoded.
   - A `RandomForestClassifier` is tuned with a small parameter grid.
   - Metrics such as ROC‑AUC, precision/recall and mean absolute error are printed.
   - Key metrics are written to `model_performance.csv` for the Streamlit dashboard.

   You can experiment with other algorithms via `train_model_lgbm.py`, `train_model_xgb.py` or `train_model_nested_cv.py`. These scripts log their metrics to the same `model_performance.csv` file so the dashboard always shows the most recent training results.

4. **Export trained pipeline**
   ```bash
   python export_model.py --algo lgbm  # or rf/xgb
   ```
   Saves the best pipeline from the selected algorithm to `f1_top3_pipeline.joblib`.

5. **Make predictions**
   ```bash
   python infer.py
   ```
   Loads `processed_data.csv` and the saved pipeline, selects the latest race and prints the three drivers with the highest predicted probability of finishing in the top 3.

6. **Streamlit dashboard**
   ```bash
   streamlit run streamlit_app.py
   ```
   Provides an interactive dashboard to select a season and race, view predicted probabilities and display basic performance information.

## Updating the model for new races

When new race data becomes available:
1. Run `fetch_f1_data.py` again. The script fetches data for all seasons starting from 2022, so rerunning it will append the latest results and weather.
2. Recreate `processed_data.csv` with `prepare_data.py`.
3. Retrain the model (`train_model.py`) and export the updated pipeline with `export_model.py --algo rf|lgbm|xgb`.
4. Use `infer.py` or the Streamlit app to see predictions for the new race.

## Data sources

- **[OpenF1](https://www.openf1.org/)** – provides minute‑level weather data and session metadata.
- **[Jolpica F1 API](https://api.jolpi.ca/ergast/f1/)** – Ergast‑compatible endpoints containing historical race, qualifying and standings information.

See `f1_api_docs.md` for example queries and field descriptions.

## Requirements

This project relies on common Python data‑science packages such as `pandas`, `scikit‑learn`, `joblib`, `lightgbm`, `xgboost` and `streamlit`. Install them via pip:
```bash
pip install pandas scikit-learn joblib lightgbm xgboost streamlit
```

## Reproducing the full pipeline

From a clean checkout:
```bash
python fetch_f1_data.py
python prepare_data.py
python train_model.py
python export_model.py --algo rf  # or lgbm/xgb
python infer.py  # shows latest race prediction
```
Optionally run the Streamlit dashboard as described above.

